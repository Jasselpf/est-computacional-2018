# Tareas {-}

```{r options, echo = FALSE, message=FALSE, error=TRUE}
knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE, error=TRUE
)
comma <- function(x) format(x, digits = 2, big.mark = ",")
options(digits = 3)

library(tidyverse)
theme_set(theme_minimal())
```

* Las tareas se envían por correo a teresa.ortiz.mancera@gmail.com con título: 
EstComp-TareaXX (donde XX corresponde al número de tarea, 01..). 

* Las tareas deben incluir código y resultados (si conocen [Rmarkdown](https://rmarkdown.rstudio.com) 
es muy conveniente para este propósito).


## 2-Transformación de datos {-}

Entrega: Lunes $27$ de agosto.

Utiliza los datos de vuelos (`flights`) para responder la siguientes preguntas.

* ¿A qué hora del día debo volar para evitar, lo más posible, retrasos de 
salida?

* Para cada destino calcula el total de minutos de retraso de salida. Para cada 
vuelo calcula su proporción del total de retrasos de su destino.

* Los retrasos suelen estar correlacionados temporalmente, incluso cuando se 
ha resuelto el problema que ocasionó los retrasos iniciales, vuelos posteriores
suelen mantener algo de retraso. Usando la función `lag()` explora como el 
retraso de salida de un vuelo se relaciona con el retraso de salida del vuelo 
anterior. Realiza una gráfica para visualizar tus hallazgos.

* Para cada destino, puedes encontrar vuelos sospechosamente rápidos o lentos?
(quizá debido a porblemas en la captura de los datos). Calcula el tiempo de 
vuelo relativo a la mediana de tiempo de vuelo a su destino. Qué vuelos se 
retrasaron más en el aire?

* Encuentra los destinos que se vuelan por al menos dos compañías (carriers).

## 3-Datos Limpios {-}

Entrega: Lunes $3$ de septiembre.

Descarga los datos [aquí](https://www.dropbox.com/s/e8wjbwpwa37ceqg/03-limpios.zip?dl=0).

En la carpeta de arriba encontrarás un archivo de excel (m_013.xls), este 
archivo contiene información de causas de mortalidad en México entre $2000$ y 
$2008$. Contesta las siguientes preguntas:

1. ¿Cuáles son las variables en esta base de datos?  
2. ¿La tabla de datos cumple con los principios de datos limpios? 
¿Qué problemas presenta?  
3. La información del archivo de excel se ha guardado también en archivos de 
texto (csv) $2001-2008$, lee y limpia los datos para que cumplan los principios de 
datos limpios. Recuerda que las modificaciones deben de ser reproducibles, para 
esto guarda tu trabajo en un script.  
4. El archivo de excel indice_marginacion.xlsx contiene el índice por entidad 
para los años $2000$ y $2010$. Realiza una gráfica donde compares la marginación por 
entidad con las tasas de mortalidad correspondientes al $2000$. Deberás unir las 
dos fuentes de información.

Observaciones:  

* Puedes filtrar/eliminar los valores a *Total* si crees que es más claro. 

* Intenta usar las funciones que estudiamos en la clase (gather, separate, 
select, filter).  

* Si aún no te sientes cómodx con las funciones de clase (y lo intentaste varias 
veces) puedes hacer las manipulaciones usando otra herramienta (incluso Excel, 
una combinación de Excel y R o cualquier software que conozcas); sin embargo, 
debes documentar tus pasos claramente, con la intención de mantener métodos 
reproducibles.

## 4-Probabilidad {-}

* Urna: 10 personas (con nombres distintos) escriben sus nombres y los ponen en 
una urna, después seleccionan un nombre (al azar). 

    - Sea A el evento en el que ninguna persona selecciona su nombre, ¿Cuál es 
    la probabilidad del evento A?  

    - Supongamos que hay 3 personas con el mismo nombre, ¿Cómo calcularías la 
    probabilidad del evento A en este nuevo experimento?

* Definimos $X$ como la variable aleatoria del 
número de juegos antes de que termine el experimento de la ruina del jugador, 
grafica la distribución de probabilidad de $X$ 
(calcula $P(X=1), P(X=2),...,P(X=100)$).

## 5-Bootstrap{-}

1. **Distribución muestral.** Consideramos la base de datos [primaria](https://raw.githubusercontent.com/tereom/est-computacional-2018/master/data/primarias.csv), 
y la columna de calificaciones de español 3^o^ de primaria (`esp_3`). 

- Selecciona una muestra de tamaño $n = 10, 100, 200$. Para cada muestra 
calcula media y el error estándar de la media usando el principio del *plug-in*:
$\hat{\mu}=\bar{x}$, y $\hat{se}(\bar{x})=\hat{\sigma}_{P_n}/\sqrt{n}$.

- Ahora aproximareos la distribución muestral, para cada tamaño de muestra $n$: 
i) simula $10,000$ muestras aleatorias, ii) calcula la media en cada muestra, iii)
Realiza un histograma de la distribución muestral de las medias (las medias del
paso anterior) iv) aproxima el error estándar calculando la desviación estándar
de las medias del paso ii.

- Calcula el error estándar de la media para cada tamaño de muestra usando la
información poblacional (ésta no es una aproximación), usa la fórmula:
$se_P(\bar{x}) = \sigma_P/ \sqrt{n}$.

- ¿Cómo se comparan los errores estándar correspondientes a los distintos 
tamaños de muestra? 

2. **Bootstrap correlación.** Nuevamente trabaja con los datos `primaria`, 
selecciona una muestra aleatoria de tamaño $100$ y utiliza el principio del 
_plug-in_ para estimar la correlación entre la calificación de $y=$español $3$ y 
la de $z=$español $6$: $\hat{corr}(y,z)$. Usa bootstrap para calcular el error
estándar de la estimación.

### Solución {-}

#### 1. Distribución muestral {-}
Suponemos que me interesa hacer inferencia del promedio de las 
calificaciones de los estudiantes de tercero de primaria en Ciudad de México.

En este ejercicio planteamos $3$ escenarios (que simulamos): 1) que tengo una 
muestra de tamaño $10$, 2) que tengo una muestra de tamaño $100$, y 3) que tengo una 
muestra de tamaño $1000$. 

- Selección de muestras:

```{r message=FALSE}
library(tidyverse)
primarias <- readr::read_csv("https://raw.githubusercontent.com/tereom/est-computacional-2018/master/data/primarias.csv")
set.seed(373783326)
muestras <- data_frame(tamanos = c(10, 100, 1000)) %>% 
    mutate(muestras = map(tamanos, ~sample(primarias$esp_3, size = .)))
```

Ahora procedemos de manera *usual* en estadística (usando fórmulas y no 
simulación), estimo la media de la muestra con el estimador *plug-in* 
$$\bar{x}={1/n\sum x_i}$$ 
y el error estándar de $\bar{x}$ con el estimador *plug-in* 
$$\hat{se}(\bar{x}) =\bigg\{\frac{1}{n^2}\sum_{i=1}^n(x_i-\bar{x})^2\bigg\}^{1/2}$$

- Estimadores *plug-in*:
```{r}
se_plug_in <- function(x){
    x_bar <- mean(x)
    n_x <- length(x)
    var_x <- 1 / n_x ^ 2 * sum((x - x_bar) ^ 2)
    sqrt(var_x)
}
muestras_est <- muestras %>% 
    mutate(
        medias = map_dbl(muestras, mean), 
        e_estandar_plug_in = map_dbl(muestras, se_plug_in)
    )
muestras_est
```

Ahora, recordemos que la distribución muestral es la distribución de una
estadística, considerada como una variable aleatoria. Usando esta definción 
podemos aproximarla, para cada tamaño de muestra, simulando:  
1) simulamos muestras de tamaño $n$ de la población,  
2) calculamos la estadística de interés (en este caso $\bar{x}$),  
3) vemos la distribución de la estadística a lo largo de simulaciones.

- Histogramas de distribución muestral y aproximación de errores estándar con 
simulación 

```{r, out.height="500px", out.height="350px"}
muestras_sims <- muestras_est %>%
    mutate(
        sims_muestras = map(tamanos, ~rerun(10000, sample(primarias$esp_3, 
            size = ., replace = TRUE))), 
        sims_medias = map(sims_muestras, ~map_dbl(., mean)), 
        e_estandar_aprox = map_dbl(sims_medias, sd)
        )
sims_medias <- muestras_sims %>% 
    select(tamanos, sims_medias) %>% 
    unnest(sims_medias) 

ggplot(sims_medias, aes(x = sims_medias)) +
    geom_histogram(binwidth = 2) +
    facet_wrap(~tamanos, nrow = 1) +
    theme_minimal()
```

Notamos que la variación en la distribución muestral decrece conforme aumenta
el tamaño de muestra, esto es esperado pues el error estándar de una media 
es $\sigma_P / \sqrt{n}$, y dado que en este ejemplo estamos calculando la media 
para la misma población el valor poblacional $\sigma_P$ es constante y solo 
cambia el denominador.

Nuestros valores de error estándar con simulación están en la columna 
`e_estandar_aprox`:

```{r}
muestras_sims %>% 
    select(tamanos, medias, e_estandar_plug_in, e_estandar_aprox)
```

En este ejercicio estamos simulando para examinar las distribuciones muestrales
y para ver que podemos aproximar el error estándar de la media usando 
simulación; sin embargo, dado que en este caso hipotético conocemos la varianza 
poblacional y la fórmula del error estándar de una media, por lo que podemos 
calcular el verdadero error estándar para una muestra de cada tamaño.

- Calcula el error estándar de la media para cada tamaño de muestra usando la
información poblacional:

```{r}
muestras_sims_est <- muestras_sims %>% 
    mutate(e_estandar_pob = sd(primarias$esp_3) / sqrt(tamanos))
muestras_sims_est %>% 
    select(tamanos, e_estandar_plug_in, e_estandar_aprox, e_estandar_pob)
```

En la tabla de arriba podemos comparar los $3$ errores estándar que calculamos, 
recordemos que de estos $3$ el *plug-in* es el único que podríamos obtener en 
un escenario real pues los otros dos los calculamos usando la población. 

Una alternativa al estimador *plug-in* del error estándar es usar *bootstrap* 
(en muchos casos no podemos calcular el error estándar *plug-in* por falta de 
fórmulas) pero podemos usar *bootstrap*: utilizamos una 
estimación de la distribución poblacional y calculamos el error estándar 
bootstrap usando simulación. Hacemos el mismo procedimiento que usamos para 
calcular *e_estandar_apox* pero sustituimos la distribución poblacional por la 
distriución empírica. Hagámoslo usando las muestras que sacamos en el primer 
paso:

```{r}
muestras_sims_est_boot <- muestras_sims_est %>% 
    mutate(
        sims_muestras_boot = map2(muestras, tamanos,
            ~rerun(10000, sample(.x, size = .y, replace = TRUE))), 
        sims_medias_boot = map(sims_muestras_boot, ~map_dbl(., mean)), 
        e_estandar_boot = map_dbl(sims_medias_boot, sd)
        )
muestras_sims_est_boot
```

Graficamos los histogramas de la distribución bootstrap para cada muestra.

```{r, out.height="500px", out.height="350px"}
sims_medias_boot <- muestras_sims_est_boot %>% 
    select(tamanos, sims_medias_boot) %>% 
    unnest(sims_medias_boot) 

ggplot(sims_medias_boot, aes(x = sims_medias_boot)) +
    geom_histogram(binwidth = 4) +
    facet_wrap(~tamanos, nrow = 1) +
    theme_minimal()
```


Y la tabla con todos los errores estándar quedaría:

```{r}
muestras_sims_est_boot %>% 
    select(tamanos, e_estandar_boot, e_estandar_plug_in, e_estandar_aprox, 
        e_estandar_pob)
```

Observamos que el estimador bootstrap del error estándar es muy similar al 
estimador plug-in del error estándar, esto es esperado pues se calcularon con la 
misma muestra y el error estándar *bootstrap* converge al *plug-in* conforme 
incrementamos el número de muestras *bootstrap*.

#### 2. Correlación {-}
2. **Bootstrap correlación.** Nuevamente trabaja con los datos `primaria`, 
selecciona una muestra aleatoria de tamaño $100$ y utiliza el principio del 
_plug-in_ para estimar la correlación entre la calificación de $y=$español $3$ y 
la de $z=$español $6$: $\hat{corr}(y,z)$. Usa bootstrap para calcular el error
estándar de la estimación.

- Selección de la muestra
```{r}
set.seed(11729874)
muestra <- sample_n(primarias, size = 100)
```

- Estimador de la correlación:

```{r}
cor(muestra$esp_3, muestra$esp_6)
```

- Error estándar con bootstrap

```{r}
cor_rep <- function(){
    muestra_boot <- sample_n(muestra, size = 100, replace = TRUE)
    cor(muestra_boot$esp_3, muestra_boot$esp_6)
}
replicaciones <- rerun(10000, cor_rep()) %>% flatten_dbl()
sd(replicaciones)
```

## 6-Cobertura de intervalos de confianza {-}

En este problema realizarás un ejercicio de simulación para comparar la 
exactitud de distintos intervalos de confianza. Simularás muestras de  
una distribución Poisson con parámetro $\lambda=2.5$ y el estadístico de interés  
es $\theta=exp(-2\lambda)$.

Sigue el siguiente proceso:

i) Genera una muestra aleatoria de tamaño $n=60$ con distribución 
$Poisson(\lambda)$, parámetro $\lambda=2.5$ (en R usa la función `rpois()`).

ii) Genera $10,000$ muestras bootstrap y calcula intervalos de confianza del 
$95\%$ para $\hat{\theta}$ usando 1) el método normal, 2) percentiles y 3) $BC_a$.

iii) Revisa si el intervalo de confianza contiene el verdadero valor del 
parámetro ($\theta=exp(-2\cdot2.5)$), en caso de que no lo contenga registra si 
falló por la izquierda (el límite inferior $exp(-2.5*\lambda)$) o falló por la 
derecha (el límite superior $exp(-2.5*\lambda)$).

a) Repite el proceso descrito $1000$ veces y llena la siguiente tabla:

Método     | \% fallo izquierda   | \% fallo derecha  | Cobertura | Longitud promedio
-----------|----------------------|-------------------|-----------|------------ 
Normal     |                      |                   |           |
Percentiles|                      |                   |           |
BC_a       |                      |                   |           |

La columna cobertura es una estimación de la cobertura del intervalo basada en 
las simulaciones, para calcularla simplemente escribe el porcentaje de los 
intervalos que incluyeron el verdadero valor del parámetro. La longitud promedio
es la longitud promedio de los intervalos de confianza bajo cada método.

b) Realiza una gráfica de páneles, en cada panel mostrarás los resultados de 
uno de los métodos (normal, percentiles y $BC_a$), el eje $x$ corresponderá al 
número de intervalo de confianza ($1,...,1000$) y en el vertical 
graficarás los límites de los intervalos, es decir graficarás $2$ líneas (usa 
`geom_line()`) una corresponderá a los límites inferiores de los intervalos, y 
otra a los superiores.

c) Repite los incisos a) y b) seleccionando muestras de tamaño $300$.

Nota: Un ejemplo en donde la cantidad $P(X=0)^2 = e^{-\lambda}$ es de interés 
es como sigue: las llamadas telefónicas a un conmutador se modelan con 
un proceso Poisson y $\lambda$ es el número promedio de llamadas por minuto, 
entonce $e^{- \lambda}$ es la probabilidad de que no se reciban llamadas en 
$1$ minuto.

### Solución {-}

```{r}
lambda <- 2.5
calcula_intervalos <- function(n = 60, B = 10000) {
    x <- rpois(n, lambda)
    theta <- exp(-2 * mean(x))
    theta_b <- rerun(B, sample(x, size = n, replace = TRUE)) %>% 
        map_dbl(~exp(-2 * mean(.)))
    bca <- bootstrap::bcanon(x, nboot = B, theta = function(y) exp(-2 * mean(y)), 
        alpha = c(0.025, 0.975))$confpoints[, 2]
    intervalos <- data_frame(metodo = c("normal", "percent", "BC_a"), 
        izq = c(theta - 1.96 * sd(theta_b), quantile(theta_b, probs = 0.025), 
            bca[1]),
        der = c(theta + 1.96 * sd(theta_b), quantile(theta_b, probs = 0.975), 
            bca[2])
    )
    list(theta = theta, intervalos = intervalos)
}
```

```{r, cache = FALSE}
set.seed(83789173)
n_sims <- 5000
# sims_intervalos_60 <- rerun(n_sims, calcula_intervalos()) 
# write_rds(sims_intervalos_60, path = "sims_intervalos_60.rds") 
sims_intervalos_60 <- read_rds("data/sims_intervalos_60.rds")
sims_intervalos_60 %>% 
    map_df(~.$intervalos) %>% 
    group_by(metodo) %>%
        summarise(
            falla_izq = 100 * sum(izq > exp(-2 * lambda)) / n_sims, 
            falla_der = 100 * sum(der < exp(-2 * lambda)) / n_sims, 
            cobertura = 100 - falla_izq - falla_der, 
            long_media = mean(der - izq),
            long_min = min(der - izq),
            long_max = max(der - izq)
            )
```

```{r, fig.width=5, fig.height=4}
intervalos_muestra <- sims_intervalos_60 %>% 
    map_df(~.$intervalos) %>% 
    mutate(sim = rep(1:n_sims, each = 3)) %>% 
    filter(sim <= 500) %>% 
    mutate(
        sim_factor = reorder(sim, der - izq), 
        sim = as.numeric(sim_factor)
        )
thetas <- sims_intervalos_60 %>% 
    map_dbl(~.$theta) 

thetas_df <- data_frame(thetas = thetas, sim = 1:n_sims) %>% 
        mutate(
        sim_factor = factor(sim, 
            levels = levels(intervalos_muestra$sim_factor)), 
        sim = as.numeric(sim_factor)
        ) %>% 
        dplyr::filter(sim <= 500) 

ggplot(intervalos_muestra) +
    geom_hline(yintercept = exp(-2 * 2.5), alpha = 0.6) +
    geom_line(aes(x = sim, y = izq), color = "red", alpha = 0.5) +
    geom_line(aes(x = sim, y = der), color = "red", alpha = 0.5) +
    geom_line(data = thetas_df, aes(x = sim, y = thetas), color = "blue", 
        alpha = 0.5) +
    facet_wrap(~ metodo, ncol = 1)
```


```{r, cache = FALSE}
set.seed(83789173)
# sims_intervalos_300 <- rerun(n_sims, calcula_intervalos(n = 300)) 
# write_rds(sims_intervalos_300, path = "sims_intervalos_300.rds") 
sims_intervalos_300 <- read_rds("data/sims_intervalos_300.rds")
sims_intervalos_300 %>% 
    map_df(~.$intervalos) %>% 
    group_by(metodo) %>%
        summarise(
            falla_izq = 100 * sum(izq > exp(-2 * lambda)) / n_sims, 
            falla_der = 100 * sum(der < exp(-2 * lambda)) / n_sims, 
            cobertura = 100 - falla_izq - falla_der, 
            longitud = mean(der - izq), 
            long_media = mean(der - izq),
            long_min = min(der - izq),
            long_max = max(der - izq)
            )
```

```{r, fig.width=5, fig.height=4, cache=TRUE}
intervalos_muestra <- sims_intervalos_300 %>% 
    map_df(~.$intervalos) %>% 
    mutate(sim = rep(1:n_sims, each = 3)) %>% 
    filter(sim <= 500) %>% 
    mutate(
        sim_factor = reorder(sim, der - izq), 
        sim = as.numeric(sim_factor)
        )
thetas <- sims_intervalos_300 %>% 
    map_dbl(~.$theta) 

thetas_df <- data_frame(thetas = thetas, sim = 1:n_sims) %>% 
        mutate(
        sim_factor = factor(sim, 
            levels = levels(intervalos_muestra$sim_factor)), 
        sim = as.numeric(sim_factor)
        ) %>% 
        dplyr::filter(sim <= 500) 

ggplot(intervalos_muestra) +
    geom_hline(yintercept = exp(-2 * 2.5), alpha = 0.6) +
    geom_line(aes(x = sim, y = izq), color = "red", alpha = 0.5) +
    geom_line(aes(x = sim, y = der), color = "red", alpha = 0.5) +
    geom_line(data = thetas_df, aes(x = sim, y = thetas), color = "blue", 
        alpha = 0.5) +
    facet_wrap(~ metodo, ncol = 1)
```

## 7-Simulación de modelos {-}
Supongamos que una compañía cambia la tecnología usada para producir una cámara, 
un estudio estima que el ahorro en la producción es de $\$5$ por unidad con un 
error estándar de $\$4$. Más aún, una proyección estima que el tamaño del mercado 
(esto es, el número de cámaras que se venderá) es de $40,000$ con un error 
estándar de $10,000$. Suponiendo que las dos fuentes de incertidumbre son 
independientes, usa simulación de variables aleatorias normales para estimar el 
total de dinero que ahorrará la compañía, calcula un intervalo de confianza. 

## 8-Simulación de modelos de regresión {-}

Los datos [beauty](https://raw.githubusercontent.com/tereom/est-computacional-2018/master/data/beauty.csv) consisten en evaluaciones de estudiantes a profesores, los 
estudiantes calificaron belleza y calidad de enseñanza para distintos cursos en 
la Universidad de Texas. Las evaluaciones de curso se realizaron al final del 
semestre y tiempo después $6$ estudiantes que no llevaron el curso realizaron los 
juicios de belleza. 

Ajustamos el siguiente modelo de regresión lineal usando las variables 
_edad_ (age), _belleza_ (btystdave), _sexo_ (female) e _inglés no es primera 
lengua_ (nonenglish) para predecir las evaluaciones del curso 
(courseevaluation).


```{r}
beauty <- readr::read_csv("https://raw.githubusercontent.com/tereom/est-computacional-2018/master/data/beauty.csv")
fit_score <- lm(courseevaluation ~ age + btystdave + female + nonenglish, 
                data = beauty)
```


1. La instructora $A$ es una mujer de $50$ años, el inglés es su primera lengua y 
tiene un puntaje de belleza de $-1$. El instructor B es un hombre de $60$ años, 

su primera lengua es el inglés y tiene un puntaje de belleza de $-0.5$. Simula
$1000$ generaciones de la evaluación del curso de estos dos instructores. En 
tus simulaciones debes incorporar la incertidumbre en los parámetros y en la
predicción. 

Para hacer las simulaciones necesitarás la distribución del vector de 
coeficientes $\beta$, este es normal con media:

```{r}
coef(fit_score)
```

y matriz de varianzas y covarianzas $\sigma^2 V$, donde $V$ es: 

```{r}
summary(fit_score)$cov.unscaled
```

y $\sigma$ se calcula como $\sigma=\hat{\sigma}\sqrt{(df)/X}$, donde X es una 
generación de una distribución $\chi ^2$ con $df$ ($458$) grados de libertad
$\hat{\sigma}$ es:

```{r}
summary(fit_score)$sigma
```

y $df$ (los grados de libertad) se obtienen:

```{r}
summary(fit_score)$df[2]
```

Una vez que obtengas una simulación del vector $\beta$ generas simulaciones 
para los profesores usando el modelo de regresión lineal y las simulaciones
de los parámetros.


+ Realiza un histograma de la diferencia entre la evaluación del curso
para $A$ y $B$. 

+ ¿Cuál es la probabilidad de que $A$ obtenga una calificación mayor?

2. En el inciso anterior obtienes simulaciones de la distribución conjunta
$p(\tilde{y},\beta,\sigma^2)$ donde $\beta$ es el vector de coeficientes de 
la regresión lineal. Para este ejercicio nos vamos a enfocar en el coeficiente
de belleza ($\beta_3$), realiza $6000$ simulaciones del modelo (como en el inciso 
anterior) y guarda las realizaciones de $\beta_3$. 

+ Genera un histograma con las simulaciones de $\beta_3$.

+ Calcula la media y desviación estándar de las simulaciones y comparalas con la 
estimación y desviación estándar del coeficiente obtenidas usando summary.

## 9-Inferencia gráfica, tamaño de muestra, bootstrap paramétrico.{-}

#### Inferencia gráfica {-}

Los datos [marg_diabetes](https://raw.githubusercontent.com/tereom/est-computacional-2018/master/data/marg_diabetes.csv) incluyen información de marginación y diabetes en 
México: 

* `ent`, `id_ent`, `mun`, `id_mun`, `cvegeo`: corresponden al estado, municipio 
y sus códigos de identificación.  

* `n_causa` es el número de muertes de adultos mayores a $65$ años a causa de
diabetes en $2015$, y `tasa_mun` la tasa correspondiente por cada $10,000$ 
habitantes.  

* `tasa_alf` (porcentaje de población alfabeta), `ind_des_hum` (índice de 
desarrollo humano), `conapo` (índice de marginación).

Utiliza los datos para explorar gráficamente la relación entre algunas de las
variables, utiliza el protocolo *lineup* para hacer inferencia gráfica.


#### Simulación para calcular tamaños de muestra {-}

Supongamos que queremos hacer una encuesta para estimar la proporción de 
hogares donde se consume refresco de manera regular, para ello se diseña un 
muestreo por conglomerados donde los conglomerados están dados por conjuntos de
hoagres de tal manera que todos los conglomerados tienen el mismo número de 
hogares. La selección de la muestra se hará en dos etapas:
    
1. Seleccionamos $J$ conglomerados de manera aleatoria.

2. En cada conglomerado seleccionames $n/J$ hogares para entrevistar.

El estimador será simplemente el porcentaje de hogares del total 
de la muestra. Suponemos que la verdadera proporción es cercana a $0.50$ y que 
la media de la proporción de interés a lo largo de los conglomerados tiene una 
desviación estándar de $0.1$.

1. Supongamos que la muestra total es de $n=1000$. ¿Cuál es la estimación del 
error estándar para la proporción estimada si $J=1,10,100,1000$?

2. El obejtivo es estimar la propoción que consume refresco en la población con 
un error estándar de a lo más $2\%$. ¿Que valores de $J$ y $n$ debemos elegir para
cumplir el objetivo al menor costo?

Los costos del levantamiento son: 
    + $50$ pesos por encuesta.
    + $500$ pesos por conglomerado
   
#### Bootstrap paramétrico {-}

1. Sean $X_1,...,X_n \sim N(\mu, 1)$. Sea $\theta = e^{\mu}$, crea una base de 
datos usando $\mu=5$ que consista de $n=100$ observaciones.

* Usa el método delta para estimar $\hat{se}$ y crea un intervalo del $95\%$ de
confianza. Usa boostrap paramétrico para crear un intervalo del $95\%$. Usa 
bootstrap no paramétrico para crear un intervalo del $95\%$. Compara tus respuestas.

* Realiza un histograma de replicaciones bootstrap para cada método, estas son
estimaciones de la distribución de $\hat{\theta}$. El método delta también nos
da una aproximación a esta distribución: $Normal(\hat{\theta},\hat{se}^2)$. 
Comparalos con la verdadera distribución de $\hat{\theta}$ (que puedes obtener 
vía simulación). ¿Cuál es la aproximación más cercana a la verdadera 
distribución?

Pista: $se(\hat{\mu}) = 1/\sqrt{n}$

### Solución {-}

#### Simulación para calcular tamaños de muestra {-}

```{r, cache = TRUE}
muestreo <- function(J, n_total = 1000) {
  n_cong <- floor(n_total / J)
  medias <- rnorm(J, 0.5, 0.1)
  medias <- ifelse(medias < 0, 0, 
      ifelse(medias > 1, 1, medias))
  resp <- rbinom(J, n_cong, medias)
  sum(resp) / n_total
}

errores <- data_frame(J = c(1, 10, 100, 1000)) %>% 
    mutate(
        sims = map(J, ~(rerun(1000, muestreo(.)) %>% flatten_dbl())), 
        error_est = map_dbl(sims, sd) %>% round(3)
            )
errores

tamano_muestra <- function(J) {
  n_total <- max(100, J)
  ee <- rerun(1000, muestreo(J = J, n_total = n_total)) %>% 
      flatten_dbl() %>% sd()
  while(ee > 0.02){
      n_total = n_total + 20
      ee <- rerun(500, muestreo(J = J, n_total = n_total)) %>% 
          flatten_dbl() %>% sd() %>% round(3)
  }
  list(ee = ee, n_total = n_total, costo = 500 * J + 50 * n_total)
}
tamanos <- c(20, 30, 40, 50, 100, 150)
costos <- map_df(tamanos, tamano_muestra)
costos$J <- tamanos
costos
ggplot(costos, aes(x = J, y = costo / 1000)) +
    geom_line() + scale_y_log10() + theme_minimal() +
    labs(y = "miles de pesos", title = "Costos")
```

## 10-Familias conjugadas {-}

#### 1. Modelo Beta-Binomial {-}

Una compañía farmacéutica afirma que su nueva medicina incrementa la 
probabilidad de concebir un niño (sexo masculino), pero aún no publican 
estudios. Supón que conduces un experimento en el cual $50$ parejas se 
seleccionan de manera aleatoria de la población, toman la medicina y conciben
un bebé, nacen $30$ niños y $20$ niñas.

a) Quieres estimar la probabilidad de concebir un niño para parejas que 
toman la medicina. ¿Cuál es una inicial apropiada? No tiene que estar centrada
en $0.5$ pues esta corresponde a personas que no toman la medicina, y la inicial 
debe reflejar tu incertidumbre sobre el efecto de la droga. 

b) Usando tu inicial de a) grafica la posterior y decide si es creíble que las
parejas que toman la medicina tienen una probabilidad de $0.5$ de concebir un
niño.

c) Supón que la farmacéutica asevera que la probabilidad de concebir un niño
cuando se toma la medicina es cercana al $60\%$ con alta certeza. Representa esta
postura con una distribución inicial $Beta(60,40)$. Comparala con la inicial de 
un escéptico que afirma que la medicina no hace diferencia, representa esta
creencia con una inicial $Beta(50,50)$. Recuerda que 
$$p(x)=Beta(z+a,N-z+b)/Beta(a,b)$$
Calcula el valor de $p(x)$ para cada modelo y el factor de Bayes (asume 
$p(M_1)=p(M_2)=0.5$).


#### 2. Otra familia conjugada {-}

Supongamos que nos interesa analizar el IQ de una muestra de estudiantes del 
ITAM y suponemos que el IQ de un estudiante tiene una distribución normal 
$x \sim N(\theta, \sigma^2)$ con $\sigma ^ 2$ conocida.
Considera que observamos el IQ de un estudiante $x$. 
La verosimilitud del modelo es:
$$p(x|\theta)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\left(-\frac{1}{2\sigma^2}(x-\theta)^2\right)$$
Realizaremos un análisis bayesiano por lo que hace falta establer una 
distribución inicial, elegimos $p(\theta)$ que se distribuya $N(\mu, \tau^2)$ 
donde elegimos los parámetros $\mu, \tau$ que mejor describan nuestras creencias
iniciales, por ejemplo si tengo mucha certeza de que el $IQ$ promedio se ubica
en $150$, elegiría $\mu=150$ y una desviación estándar chica por ejemplo 
$\tau = 5$. Entonces la distribución inicial es:

$$p(\theta)=\frac{1}{\sqrt{2\pi\tau^2}}exp\left(-\frac{1}{2\tau^2}(\theta-\mu)^2\right)$$
Calcula la distribución posterior $p(\theta|x) \propto p(x|\theta)p(\theta)$, 
usando la inicial y verosimilitud que definimos arriba. Una vez que realices la
multiplicación debes identificar el núcleo de una distribución Normal, 
¿cuáles son sus parámetros (media y varianza)?

## 11-Metropolis {-}

Regresamos al ejercicio de IQ de la tarea anterior, en ésta hiciste cálculos 
para el caso de una sola observación. En este ejercicio consideramos el caso en 
que observamos una muestra $x=\{x_1,...,x_N\}$, y utilizaremos Metrópolis 
para obtener una muestra de la distribución posterior.

a) Crea una función $prior$ que reciba los parámetros $\mu$ y $\tau$ que definen 
tus creencias del parámetro desconocido $\theta$ y devuelva $p(\theta)$, donde 
$p(\theta)$ tiene distriución $N(\mu, \sigma^2)$

```{r, eval=FALSE}
prior <- function(mu, tau{
  function(theta){
    ... # llena esta parte
  }
}
```

b) Utiliza la función que acabas de escribir para definir una distribución 
inicial con parámetros $\mu = 150$ y $\tau = 15$, llámala _mi\_prior_.

  Ya que tenemos la distribución inicial debemos escribir la verosimilitud, en 
  este caso la verosimilitud es:

$$p(x|\theta, \sigma^2)=\frac{1}{(2\pi\sigma^2)^{N/2}}exp\left(-\frac{1}{2\sigma^2}\sum_{j=1}^{N}(x_j-\theta)^2\right)$$
$$=\frac{1}{(2\pi\sigma^2)^{N/2}}exp\left(-\frac{1}{2\sigma^2}\bigg(\sum_{j=1}^{N}x_j^2-2\theta\sum_{j=1}^{N} x_j + N\theta^2 \bigg) \right)$$

Recuerda que estamos suponiendo varianza conocida, supongamos que la 
desviación estándar es $\sigma=20$.

$$p(x|\theta)=\frac{1}{(2\pi (20^2))^{N/2}}exp\left(-\frac{1}{2 (20^2)}\bigg(\sum_{j=1}^{N}x_j^2-2\theta\sum_{j=1}^{N} x_j + N\theta^2 \bigg) \right)$$

c) Crea una función $likeNorm$ en R que reciba la desviación estándar, la suma 
de los valores observados $\sum x_i$,  la suma de los valores al cuadrado 
$\sum x_i^2$ y el número de observaciones $N$ la función devolverá la 
función de verosimilitud  (es decir va a regresar una función que depende 
únicamente de $\theta$).

```{r, eval = FALSE}
# S: sum x_i, S2: sum x_i^2, N: número obs.
likeNorm <- function(S, S2, N){
  function(theta){
    ...  # llena esta parte
  }
}
```

d) Supongamos que aplicamos un test de IQ a $100$ alumnos y observamos que la suma
de los puntajes es $13,300$, es decir $\sum x_i=13,000$ y $\sum x_i^2=1,700,000$.
Utiliza la función que acabas de escribir para definir la función de 
verosimilitud condicional a los datos observados, llámala _mi\_like_.

e) La distribución posterior no normalizada es simplemente el producto de 
la inicial y la posterior:

```{r}
postRelProb <- function(theta){
  mi_like(theta) * mi_prior(theta)
}
```

Utiliza Metropolis para obtener una muestra de valores representativos de la
distribución posterior de $\theta$. Para proponer los saltos utiliza una 
$Normal(0, 5)$.

f) Grafica los valores de la cadena para cada paso.

g)  Elimina los valores correspondientes a la etapa de calentamiento y realiza
un histograma de la distribución posterior.

h)  Si calcularas la posterior de manera analítica obtendrías que $p(x|\theta)$
es normal con media:
$$\frac{\sigma^2}{\sigma^2 + N\tau^2}\mu + \frac{N\tau^2}{\sigma^2 + N \tau^2}\bar{x}$$
y varianza
$$\frac{\sigma^2 \tau^2}{\sigma^2 + N\tau^2}$$

donde $\bar{x}=1/N\sum_{i=1}^N x_i$ es la media de los valores observados.
Realiza simulaciones de la distribución posterior calculada de manera analítica
y comparalas con el histograma de los valores generados con Metropolis.

i) ¿Cómo utilizarías los parámetros $\mu, \tau^2$ para describir un escenario 
donde sabes poco del verdadero valor de la media $\theta$?

### Solución {-}

```{r}
prior <- function(mu = 100, tau = 10){
  function(theta){
    dnorm(theta, mu, tau)
  }
}
mu <- 150
tau <- 15
mi_prior <- prior(mu, tau)
```

c) 
```{r}
# S: sum x_i, S2: sum x_i^2, N: número obs., sigma: desviación estándar (conocida)
S <- 13000 
S2 <- 1700000
N <- 100

# sigma2 <- S2 / N - (S / N) ^ 2
sigma <- 20
  
likeNorm <- function(S, S2, N, sigma = 20){
  # quitamos constantes
  sigma2 <-  sigma ^ 2
  function(theta){
    exp(-1 / (2 * sigma2) * (S2 - 2 * theta * S + 
        N * theta ^ 2))
  }
}
```

d)
```{r}
mi_like <- likeNorm(S = S, S2 = S2, N = N, sigma = sigma)
mi_like(150)
```

e)
```{r}
postRelProb <- function(theta){
  mi_like(theta) * mi_prior(theta)
}

# para cada paso decidimos el movimiento de acuerdo a la siguiente función
caminaAleat <- function(theta, sd_prop = 5){ # theta: valor actual
  salto_prop <- rnorm(n = 1, sd = sd_prop) # salto propuesto
  theta_prop <- theta + salto_prop # theta propuesta
  u <- runif(1) 
  p_move = min(postRelProb(theta_prop) / postRelProb(theta), 1) # prob mover
  if(p_move  > u){
    return(theta_prop) # aceptar valor propuesto
  }
  else{
    return(theta) # rechazar
  }
}


pasos <- 10000
camino <- numeric(pasos) # vector que guardará las simulaciones
camino[1] <- 90 # valor inicial

rechazo = 0
# Generamos la caminata aleatoria
for (j in 2:pasos){
  camino[j] <- caminaAleat(camino[j - 1])
  rechazo <- rechazo + 1 * (camino[j] == camino[j - 1]) 
}

rechazo / pasos
caminata <- data.frame(pasos = 1:pasos, theta = camino)


```

f)

```{r, out.height="300px"}
ggplot(caminata[1:2000, ], aes(x = pasos, y = theta)) +
  geom_point(size = 0.8) +
  geom_path(alpha = 0.3) 
```

g)

```{r, out.height="300px"}
ggplot(filter(caminata, pasos > 2000), aes(x = theta)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.8) 
```

h)

```{r, out.height="200px"}
ggplot(filter(caminata, pasos > 2000), aes(x = theta)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.7) + 
  stat_function(fun = dnorm, args = list(mean = 130.3, sd = 1.98), color = "red")
  
sigma ^ 2 * mu / (sigma ^ 2 + N * tau ^ 2) + tau ^ 2 * S / (sigma^2  + N * tau^2) # media
sigma ^ 2 * tau ^ 2 / (sigma ^ 2 + N * tau ^ 2)
```

i) Elegir varianza grande nos daría una inicial poco informativa.

## 12-MCMC convergencia {-}

Implementaremos un modelo de regresión en JAGS, la base de datos que
usaremos contiene información de mediciones de radón (activity)
y del suelo en el que se hicieron las mediciones (floor = $0$ casas con
sótano, floor = $1$ casas sin sótano), las mediciones corresponden a $919$
hogares muestreados de $85$ condados de Minnesota. El objetivo es
construir un modelo de regresión en el que la medición de radón es la
variable dependiente y el tipo de suelo es la covariable.

El modelo es como sigue:

$$y_i \sim N(\alpha + \beta x_i, \sigma^2)$$

La distribuciones iniciales que usaremos son:
$$\beta \sim N(0, 1000)$$
$$\sigma^2 \sim U(0, 1000)$$

```{r, message=FALSE, warning=FALSE, eval=FALSE}
modelo_regresion.txt <-
    '
    model{
      for(i in 1 : n) {
        y[i] ~ dnorm(y.hat[i], tau.y) 
        y.hat[i] <- a + b * x[i]
      }
      a ~ dnorm(0, 0.01)
      b ~ dnorm(0, 0.001)
      tau.y <- pow(sigma.y, -2)
      sigma.y ~ dunif(0, 100)
    }
    '
cat(modelo_regresion.txt, file = 'modelo_regresion.bugs')

# cargamos los datos con load radon
radon <- readr::read_csv("data/radon.csv")

# Iniciamos preparando los datos para el análisis, trabajaremos en
# escala logarítmica, hay algunos casos con medición cero, para éstos
# hacemos una pequeña correción redondeándolos a 0.1.
y <- log(ifelse (radon$activity == 0, 0.1, radon$activity))
n <- nrow(radon)
x <- radon$floor

# jags
radon1_data <- list("n", "y", "x")
radon1_parameters <- c("a", "b", "sigma.y")
```

El ejercicio consiste en que utilces la función `jags()` definiendo valores 
inciales, número de cadenas, número de iteraciones y etapa de calentamiento. 
Asegurate de alcanzar convergencia y describe los diagnósticos que utilizaste 
para concluir que se convergió a la distribución posterior.

Instalar Stan y rstan, instrucciones [aquí](http://mc-stan.org/users/interfaces/rstan.html).

## 13-Modelos jerárquicos {-}

En este ejercicio definirás un modelo jerárquico para la incidencia de tumores
en grupos de conejos a los que se suministró una medicina. Se realizaron 71
experimentos distintos utilizando la misma medicina. 

Considerando que cada conejo proviene de un experimento distinto, se desea
estudiar $\theta_j$, la probabilidad de desarrollar un tumor en el 
$j$-ésimo grupo, este parámetro variará de grupo a grupo.

Denotaremos $y_{ij}$ la observación en el $i$-ésimo conejo perteneciente al 
$j$-ésimo experimento, $y_{ij}$ puede tomar dos valores: 1 indicando que el 
conejo desarrolló tumor y 0 en el caso contrario, por tanto la verosimilitud 
sería:

$$y_{ij} \sim Bernoulli(\theta_j)$$

Adicionalmente se desea estimar el efecto medio de la medicina a lo largo de
los grupos $\mu$, por lo que utilizaremos un modelo jerárquico como sigue:

$$\theta_j \sim Beta(a, b)$$

donde 

$$a=\mu \kappa$$
$$b=(1-\mu)\kappa$$

Finalmente asignamos una distribución a los hiperparámetros $\mu$ y $\kappa$,

$$\mu \sim Beta(A_{\mu}, B_{\mu})$$

$$\kappa \sim Gamma(S_{\kappa}, R_{\kappa})$$

1. Si piensas en este problema como un lanzamiento de monedas, ¿a qué 
corresponden las monedas y los lanzamientos?

2. Los datos en el archivo `rabbits.RData` contienen las observaciones de los 
71 experimentos, 
cada renglón corresponde a una observación. 
    + Utiliza JAGS o Stan para ajustar un modelo jerárquico como el descrito 
    arriba y usando una inicial $Beta(1, 1)$ y una $Gamma(1, 0.1)$ para $\mu$ y
    $\kappa$ respectivamente. 
    + Realiza un histograma de la distribución posterior de $\mu$, $\kappa$. 
    Comenta tus resultados.

3. Ajusta un nuevo modelo utilizando una iniciales $Beta(10, 10)$ y 
$Gamma(0.51, 0.01)$ para $\mu$ y $\kappa$ (lo demás quedará igual). 
    + Realiza una gráfica con las medias posteriores de los parámetros 
    $\theta_j$ bajo los dos escenarios de distribuciones iniciales: en el eje 
    horizontal grafica las medias posteriores del modelo ajustado en el paso
    anterior y en el eje vertical las medias posteriores del segundo modelo . 
    ¿Cómo se comparan? ¿A qué se deben las diferencias?
    
## 14-Ejercicios clase modelos jerárquicos {-}
Implementaremos varios modelos en JAGS o Stan, la 
base de datos que usaremos contiene información de mediciones de radón 
(activity) y del suelo en el que se hicieron las mediciones (floor = $0$ casas 
con sótano, floor = $1$ casas sin sótano), las mediciones corresponden a $919$ 
hogares muestreados de $85$ condados de Minnesota, (este es ejercicio se tomó 
de @gelman-hill). El objetivo es construir un 
modelo de regresión en el que la medición de radón es la variable independiente 
y el tipo de suelo es la covariable.

1. Iniciaremos con un modelo de regresión de unidades iguales, este modelo 
ignora la variación en los niveles de radón entre los condados.

$$y_i \sim N(\alpha + \beta x_i, \sigma_y^2) $$

```{r}
# preparación de los datos
library(R2jags)

radon <- read.csv("data/radon.csv")
# creamos variable de condado como entrero consecutivo para iterar 
radon$county <- as.numeric(factor(radon$cntyfips))
y <- log(ifelse (radon$activity == 0, 0.1, radon$activity))
n <- nrow(radon)
x <- radon$floor
J <- length(unique(radon$county))
county <- radon$county
```


```{r}
modelo_1 <- 
'
model{
  for(i in 1 : n) {
    y[i] ~ dnorm(y.hat[i], tau.y) 
    y.hat[i] <- a + b * x[i]
  }
  a ~ dnorm(0, 0.0001)
  b ~ dnorm(0, 0.0001)
  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif(0, 100)
}
'
cat(modelo_1, file = 'modelo_1.bugs')
```

```{r}
# llamar a jags
radon_1_data <- list(n = n, y = y, x = x)
radon_1_inits <- function(){
    list(a = rnorm(1), b = rnorm(1), sigma.y = runif(1))}
radon_1_parameters <- c("a", "b", "sigma.y")

radon_1_jags <- jags(data = radon_1_data, inits= radon_1_inits, 
    parameters.to.save = radon_1_parameters, model.file = "modelo_1.bugs", 
    n.iter = 1000)
radon_1_jags
```

2. Después pasamos a un modelo de unidades independientes, en este simplemente 
incluímos indicadoras a nivel condado.

$$y_i \sim N(\alpha_{j[i]} + \beta x_i, \sigma_y^2)$$
```{r}
modelo_2 <- 
'
model{
  for(i in 1 : n) {
    y[i] ~ dnorm(y.hat[i], tau.y) 
    y.hat[i] <- a[county[i]] + b * x[i]
  }
  b ~ dnorm(0, 0.001)
  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif(0, 100)
  for(j in 1:J){
    a[j] ~ dnorm(0, 0.001)
  }
}
'
cat(modelo_2, file = 'modelo_2.bugs')
```


```{r}
radon_2_data <- list(n = n, J = J, y = y, county = county, x = x)
radon_2_inits <- function(){
    list(a = rnorm(J), b = rnorm(1), sigma.y = runif(1))}
radon_2_parameters <- c("a", "b", "sigma.y")

radon_2_jags <- jags(data = radon_2_data, inits= radon_2_inits, 
    parameters.to.save = radon_2_parameters, model.file = "modelo_2.bugs", 
    n.iter = 1000)
plot(radon_2_jags)
```


3. Añadimos una estructura jerárquica al modelo:
$$y_i \sim N(\alpha_{j[i]} + \beta x_i, \sigma_y^2) $$
$$\alpha_j \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)$$

```{r}
modelo_3 <- 
'
model{
  for(i in 1 : n) {
    y[i] ~ dnorm(y.hat[i], tau.y) 
    y.hat[i] <- a[county[i]] + b * x[i]
  }
  b ~ dnorm(0, 0.0001)
  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif(0, 100)
  for(j in 1:J){
    a[j] ~ dnorm(mu.a, tau.a)
  }
  mu.a ~ dnorm(0, 0.0001)
  tau.a <- pow(sigma.a, -2)
  sigma.a ~ dunif(0, 100)
}
'
cat(modelo_3, file = 'modelo_3.bugs')
```

```{r}
radon_inits <- function(){
    list(a = rnorm(J), b = rnorm(1), mu.a = rnorm(1), sigma.y = runif(1), 
        sigma.a = runif(1))}
radon_parameters <- c("a", "b", "mu.a", "sigma.y", "sigma.a")

radon_3_jags <- jags(data = radon_2_data, inits= radon_inits, 
    parameters.to.save = radon_parameters, model.file = "modelo_3.bugs", 
    n.iter = 1000, n.chains = 2)

plot(radon_3_jags)

```


4. Incorporamos una covariable $u_j$ a nivel grupo, en este caso elegiremos una 
medición de uranio a nivel condado (Uppm).

$$y_i \sim N(\alpha_{j[i]} + \beta x_i, \sigma_y^2) $$
$$\alpha_j \sim N(\gamma_0 + \gamma_{1}u_j, \sigma_{\alpha}^2)$$

```{r}
modelo_4 <- 
'
model{
  for(i in 1 : n) {
    y[i] ~ dnorm(y.hat[i], tau.y) 
    y.hat[i] <- a[county[i]] + b * x[i]
  }
  b ~ dnorm(0, 0.0001)
  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif(0, 100)

  for(j in 1:J){
    a[j] ~ dnorm(a.hat[j], tau.a)
    a.hat[j] <- g.0 + g.1 * u[j]
}
  g.0 ~ dnorm(0, 0.0001)
  g.1 ~ dnorm(0, 0.0001)
  tau.a <- pow(sigma.a, -2)
  sigma.a ~ dunif(0, 100)
}
'
cat(modelo_4, file = 'modelo_4.bugs')
```


```{r}
uranium <- unique(cbind(county, radon$Uppm))
u <- uranium[, 2]
radon_4_data <- list(n = n, J = J, y = y, county = county, x = x, u = u)
radon_4_inits <- function(){
    list(a = rnorm(J), b = rnorm(1), g.0 = rnorm(1), g.1 = rnorm(1), 
        sigma.y = runif(1), sigma.a = runif(1))}
radon_4_parameters <- c("a", "b", "g.0", "g.1", "sigma.y", "sigma.a")

radon_4_jags <- jags(data = radon_4_data, inits= radon_4_inits, 
    parameters.to.save = radon_4_parameters, model.file = "modelo_4.bugs", 
    n.iter = 1000)
plot(radon_4_jags)
```


5. Utiliza el modelo anterior para predecir el valor de radón para una nueva 
casa sin sótano (floor = 1) en el condado 26.

```{r}
a26 <- radon_4_jags$BUGSoutput$sims.list$a[, 26]
b <- radon_4_jags$BUGSoutput$sims.list$b
sigma.y <- radon_4_jags$BUGSoutput$sims.list$sigma.y
n.sims <- length(b)
y_tilde <- rnorm(n.sims, a26 + b * 1, sigma.y)
sd_y_tilde <- sd(y_tilde)
# intervalo
c(mean(y_tilde) - 1.96 *sd_y_tilde, mean(y_tilde) + 1.96 *sd_y_tilde)
```

6. Utiliza el modelo anterior para predecir el valor de radón para una nueva 
casa sin sótano (floor = 1) en un condado nuevo con nivel de uranio $2$.

```{r}
g0 <- radon_4_jags$BUGSoutput$sims.list$g.0
g1 <- radon_4_jags$BUGSoutput$sims.list$g.1
sigma.a <- radon_4_jags$BUGSoutput$sims.list$sigma.a
a.new <- rnorm(n.sims, g0+g1, sigma.a)
y_tilde <- rnorm(n.sims, a.new + b * 1, sigma.y)
sd_y_tilde <- sd(y_tilde)
c(mean(y_tilde) - 1.96 * sd_y_tilde, mean(y_tilde) + 1.96 *sd_y_tilde)

